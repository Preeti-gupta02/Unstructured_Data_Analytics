{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fce937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/denverbaumgartner/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# Essentials\n",
    "import base64\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datapane as dp\n",
    "#dp.login(token='INSERT_TOKEN_HERE')\n",
    "# Gensim and LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import pyLDAvis.gensim_models\n",
    "# NLP stuff\n",
    "import contractions\n",
    "import demoji\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "import spacy\n",
    "# Plotting tools\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Miscellaneous\n",
    "from sklearn.manifold import TSNE\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bef3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parser(df): \n",
    "    # take in a dataframe and subset the dataframe into individual dataframes based off of the 'movie_id' column\n",
    "    # return a list of dataframes\n",
    "    df_list = []\n",
    "    for i in df['movie_id'].unique():\n",
    "        df_list.append(df[df['movie_id'] == i])\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf630481",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dfs = data_parser(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text_col):\n",
    "    \"\"\"This function will apply NLP preprocessing lambda functions over a pandas series such as df['text'].\n",
    "       These functions include converting text to lowercase, removing emojis, expanding contractions, removing punctuation,\n",
    "       removing numbers, removing stopwords, lemmatization, etc.\"\"\"\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w.lower() for w in x.split()]))\n",
    "    \n",
    "    # remove emojis\n",
    "    text_col = text_col.apply(lambda x: demoji.replace(x, \"\"))\n",
    "    \n",
    "    # expand contractions  \n",
    "    text_col = text_col.apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))\n",
    "\n",
    "    # remove punctuation\n",
    "    text_col = text_col.apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))\n",
    "    \n",
    "    # remove numbers\n",
    "    text_col = text_col.apply(lambda x: ' '.join(re.sub(\"[^a-zA-Z]+\", \" \", x).split()))\n",
    "\n",
    "    # remove stopwords\n",
    "    stopwords = [sw for sw in list(nltk.corpus.stopwords.words('english')) + ['thing'] if sw not in ['not']]\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w for w in x.split() if w not in stopwords]))\n",
    "\n",
    "    # lemmatization\n",
    "    text_col = text_col.apply(lambda x: ' '.join([WordNetLemmatizer().lemmatize(w) for w in x.split()]))\n",
    "\n",
    "    # remove short words\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w.strip() for w in x.split() if len(w.strip()) >= 3]))\n",
    "\n",
    "    return text_col\n",
    "\n",
    "for df in movie_dfs:\n",
    "    df['text'] = preprocess(df['text'])\n",
    "    df.rename(columns={'text': 'review'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence).encode('utf-8'), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for extracting words from df, create bigrams and trigrams\n",
    "data_words = []\n",
    "for df in movie_dfs:\n",
    "    data_words.append(list(sent_to_words(df['text'].to_list())))\n",
    "\n",
    "bigrams = []\n",
    "for data in data_words:\n",
    "    bigrams.append(gensim.models.Phrases(data, min_count=5, threshold=25))\n",
    "\n",
    "trigrams = []\n",
    "for data in data_words:\n",
    "    trigrams.append(gensim.models.Phrases(bigrams[0][data], min_count=5, threshold=25))\n",
    "\n",
    "bigrams_mod = [gensim.models.phrases.Phraser(bigram) for bigram in bigrams] \n",
    "trigrams_mod = [gensim.models.phrases.Phraser(trigram) for trigram in trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c9f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigrams_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigrams_mod[bigrams_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c1c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create one final set list of dataframes for each of the movies and apply the functions above\n",
    "data_no_stopwords = []\n",
    "for data in data_words:\n",
    "    data_no_stopwords.append(remove_stopwords(data))\n",
    "\n",
    "data_bigrams = []\n",
    "for data in data_no_stopwords:\n",
    "    data_bigrams.append(make_bigrams(data))\n",
    "\n",
    "data_preprocessed = []\n",
    "for data in data_bigrams:\n",
    "    data_preprocessed.append(make_trigrams(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4dea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary and corpus needed for topic modelling \n",
    "\n",
    "# dictionary is a mapping of words to their integer ids\n",
    "all_id2word = []\n",
    "for data in data_preprocessed:\n",
    "    all_id2word.append(gensim.corpora.Dictionary(data))\n",
    "\n",
    "for data in all_id2word:\n",
    "    data.filter_extremes(no_below=15, no_above=0.4, keep_n=80000)\n",
    "\n",
    "# corpus is a mapping of word_id to their frequency in the document\n",
    "all_corpus = []\n",
    "for data in data_preprocessed:\n",
    "    for words in all_id2word: \n",
    "        all_corpus.append([words.doc2bow(text) for text in data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build the lda model for all movies based upon the corpus and dictionary created for each movie\n",
    "# this will create the topic models for each movie\n",
    "all_models = {}\n",
    "movie_titles = df['movie_id'].unique()\n",
    "for i in range(len(movie_titles)):\n",
    "    all_models[movie_titles[i]] = gensim.models.ldamodel.LdaModel(corpus=all_corpus[i],\n",
    "                                           id2word=all_id2word[i],\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "name": "python37364bitbasecondad5fd69e5360a491f97b41f6b1a972826"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
